### 内存屏障
1、 cpu乱序执行和编译器重排
```
代码顺序并不是真正的执行顺序，只要有空间提高性能，CPU和编译器可以进行各种优化。

// cpu乱序执行
    在一个固定长度的执行队列中，寻找可以同时执行的指令。这个过程只需考虑指令间是否有依赖关系，不需要理解程序的意图

// 编译器重排
    比处理器的范围更大，能在很大范围内进行代码分析,从而做出更优的策略,充分利用处理器的乱序执行功能.

```
2、cpu缓存架构
```
http://www.wowotech.net/kernel_synchronization/memory-barrier.html

                            Main Memory
                                ↑
                                ↓
                        System Bus Interface 
                                ↑               
                                ↓               
            +--------------- L2 Cache <-----------------------+    
            |                   ↑                             |  
            ↓                   ↓                             |  
    L1 Instruction Cache     L1 Data Cache                    | 
            ↑                 ↑         ↑                     |
            |          Load Buffer   Store Buffer ---> Write-Combining Buffers 
            |                 ↑         ↑
            |                 |         |
            +------------->Execution Units
                              Registers


// cache
    cpu中的cache是一行一行的，每行可存储多个变量，每行都有自己的cache状态

// MESI协议
    https://www.jianshu.com/p/94200fc2d3f1
    来确保硬件级别Cache一致性（Cache Coherence）
    > cache状态
        M（修改, Modified）: 本地处理器已经修改缓存行, 即是脏行, 它的内容与内存中的内容不一样. 并且此cache只有本地一个拷贝（专有）。
        E（专有, Exclusive）: 缓存行内容和内存中的一样, 而且其它处理器都没有这行数据。
        S（共享, Shared）: 缓存行内容和内存中的一样, 有可能其它处理器也存在此缓存行的拷贝。
        I（无效, Invalid）: 缓存行失效, 不能使用。
        modified状态和exclusive状态都是独占该cacheline, 但是modified状态下，cacheline的数据是dirty的，而exclusive状态下，cacheline中的数据和memory中的数据是一致的
    > cache操作
        local read（LR）：读本地cache中的数据；
        local write（LW）：将数据写到本地cache；
        remote read（RR）：其他核心发生read；
        remote write（RW）：其他核心发生write；
    > cache状态转换
        > 初始场景
            在最初的时候，所有CPU中都没有数据，某一个CPU发生读操作，此时必然发生cache miss，数据从主存中读取到当前CPU的cache，状态为E（独占，只有当前CPU有数据，且和主存一致），此时如果有其他CPU也读取数据，则状态修改为S（共享，多个CPU之间拥有相同数据，并且和主存保持一致），如果其中某一个CPU发生数据修改，那么该CPU中数据状态修改为M（拥有最新数据，和主存不一致，但是以当前CPU中的为准），其他拥有该数据的核心通过缓存控制器监听到remote write行文，然后将自己拥有的数据的cache line状态修改为I（失效，和主存中的数据被认为不一致，数据不可用应该重新获取）。
        > modify
            当前CPU中数据的状态是modify，表示当前CPU中拥有最新数据，虽然主存中的数据和当前CPU中的数据不一致，但是以当前CPU中的数据为准；
            LR：此时如果发生local read，即当前CPU读数据，直接从cache中获取数据，拥有最新数据，因此状态不变；
            LW：直接修改本地cache数据，修改后也是当前CPU拥有最新数据，因此状态不变；
            RR：因为本地内存中有最新数据，当本地cache控制器监听到总线上有RR发生的时，必然是其他CPU发生了读主存的操作，此时为了保证一致性，当前CPU应该将数据写回主存，而随后的RR将会使得其他CPU和当前CPU拥有共同的数据，因此状态修改为S；
            RW：同RR，当cache控制器监听到总线发生RW，当前CPU会将数据写回主存，因为随后的RW将会导致主存的数据修改，因此状态修改成I；
        > exclusive
            当前CPU中的数据状态是exclusive，表示当前CPU独占数据（其他CPU没有数据），并且和主存的数据一致；
            LR：从本地cache中直接获取数据，状态不变；
            LW：修改本地cache中的数据，状态修改成M（因为其他CPU中并没有该数据，因此不存在共享问题，不需要通知其他CPU修改cache line的状态为I）；
            RR：本地cache中有最新数据，当cache控制器监听到总线上发生RR的时候，必然是其他CPU发生了读取主存的操作，而RR操作不会导致数据修改，因此两个CPU中的数据和主存中的数据一致，此时cache line状态修改为S；
            RW：同RR，当cache控制器监听到总线发生RW，发生其他CPU将最新数据写回到主存，此时为了保证缓存一致性，当前CPU的数据状态修改为I；
        > shared
            当前CPU中的数据状态是shared，表示当前CPU和其他CPU共享数据，且数据在多个CPU之间一致、多个CPU之间的数据和主存一致；
            LR：直接从cache中读取数据，状态不变；
            LW：发生本地写，并不会将数据立即写回主存，而是在稍后的一个时间再写回主存，因此为了保证缓存一致性，当前CPU的cache line状态修改为M，并通知其他拥有该数据的CPU该数据失效，其他CPU将cache line状态修改为I；
            RR：状态不变，因为多个CPU中的数据和主存一致；
            RW：当监听到总线发生了RW，意味着其他CPU发生了写主存操作，此时本地cache中的数据既不是最新数据，和主存也不再一致，因此当前CPU的cache line状态修改为I；
        > invalid
            当前CPU中的数据状态是invalid，表示当前CPU中是脏数据，不可用，其他CPU可能有数据、也可能没有数据；
            LR：因为当前CPU的cache line数据不可用，因此会发生读内存，此时的情形如下。
                A. 如果其他CPU中无数据则状态修改为E；
                B. 如果其他CPU中有数据且状态为S或E则状态修改为S；
                C. 如果其他CPU中有数据且状态为M，那么其他CPU首先发生RW将M状态的数据写回主存并修改状态为S，随后当前CPU读取主存数据，也将状态修改为S；
            LW：因为当前CPU的cache line数据无效，因此发生LW会直接操作本地cache，此时的情形如下。
                A. 如果其他CPU中无数据，则将本地cache line的状态修改为M；
                B. 如果其他CPU中有数据且状态为S或E，则修改本地cache，通知其他CPU将数据修改为I，当前CPU中的cache line状态修改为M；
                C. 如果其他CPU中有数据且状态为M，则其他CPU首先将数据写回主存，并将状态修改为I，当前CPU中的cache line转台修改为M；
            RR：监听到总线发生RR操作，表示有其他CPU读取内存，和本地cache无关，状态不变；
            RW：监听到总线发生RW操作，表示有其他CPU写主存，和本地cache无关，状态不变；

    > 这个协议有两个行为的执行成本比较大：
        一个是将某个Cache Line标记为Invalid状态
        一个是当某Cache Line当前状态为Invalid时写入新的数据
    > 所以CPU通过Store Buffer和Invalidate Queue组件来降低这类操作的延时

// 示例
    cache line状态           动作
    invalid
       |                cpu0读取变量，发生cache miss(cold miss)
       ↓                从memory加载到cache0的cache line中
    shared                  ↓
       |                此时cpu0想要修改cache0 
       ↓                cpu0发送read invalidate，加载了cache0的cache line
    exclusive(cpu0)         
    invalid(其他cpu)        ↓    
       ↓                对cache进行写操作
    modified(cpu0)          ↓
                        当该cache line被替换出cache的时候，modified状态的cacheline需要write back到memory中，而exclusive状态不需要


// store buffer
    当该cache line没有被替换出cache的时候，其他cpu读取该共享变量，由于之前已经变成invalid，则发生cache miss(communiction miss)。由于cpu0的cache line是modified状态，它必须响应这个读得操作
    从一个CPU的cacheline传递数据到另外一个CPU的cacheline是非常消耗时间的
    每个cpu写操作不必等到cache line被加载（exclusive状态），而是直接写到store buffer中然后去干其他的活。在CPU n的cache line把数据传递到其cache 0的cacheline之后，硬件将store buffer中的内容写入cache line。

// Invalidate Queue



```
3、 内存屏障
```
https://www.jianshu.com/p/64240319ed60
http://ifeve.com/linux-memory-barriers/
http://ifeve.com/memory-barriers-or-fences/
https://zhuanlan.zhihu.com/p/43526907

(1) 可见性与重排序
    1) 可见性
        cpu不同级别的Cache缓存，可以提高性能，但是多核时代会导致一致性问题
        例如：
            Core0与Core1命中了内存中的同一个地址，那么各自的L1 Cache会缓存同一份数据的副本
            Core0修改自己的L1 Cache，而Core1的L1 Cache的值没变
    2) 重排序
        > 真·重排序
            编译器、底层硬件（CPU等）出于“优化”的目的，按照某种规则将指令重新排序
            指令被发送到一个指令序列（也称执行缓冲区或者保留站）中。指令将在序列中等待，直到它的数据运算对象是可以获取的。然后被送到执行单元
        > 伪·重排序
            由于缓存同步顺序等问题，看起来指令被重排序了
        > 来源
            编译器编译时的优化 真·重排序
            处理器执行时的乱序优化 真·重排序
            缓存同步顺序（导致可见性问题）伪·重排序


(2) 什么是内存屏障
    一个系统中，CPU和其它硬件可以使用各种技巧来提高性能，包括内存操作的重排、延迟和合并；预取；推测执行分支以及各种类型的缓存。内存屏障是用来禁用或抑制这些技巧的，使代码稳健地控制多个CPU和(或)设备的交互
    不同的CPU架构上内存屏障的实现非常不一样，内存屏障是cpu技术

(3) 内存屏障功能
    1) 屏障的两边的所有指令都是正确的程序顺序，而保持程序顺序的外部可见性
    2) 实现内存数据可见性，确保内存数据会同步到CPU缓存子系统
    注意：可见性可以认为是最弱的“一致性”（弱一致），只保证用户见到的数据是一致的，但不保证任意时刻，存储的数据都是一致的（强一致）。下文会讨论“缓存可见性”问题，部分文章也会称为“缓存一致性”问题。

(4) 重排序场景
    1) 编译器编译时的优化
        volatile禁用编译器优化
        解决了编译器重排
    2) 处理器执行时的乱序优化
        特殊的指令
        解决了cpu重排
    3) 缓存同步顺序（导致可见性问题）
        MESI协议，解决CPU缓存层面的问题
        解决了内存可见性
    注意：C++中voldatile等于插入编译器级别屏障，因此并不能阻止CPU硬件级别导致的重排。C++11 中volatile语义没有任何变化，不过提供了std::atomic工具可以真正实现原子操作，而且默认加入了内存屏障

(5) 内存屏障
    1) 屏障类型
        > LoadLoad Barriers
            指令示例: Load1;LoadLoad;Load2	
            说明: 该屏障确保Load1数据的装载先于Load2及其后所有装载指令的的操作
        > StoreStore Barriers
            指令示例: Store1;StoreStore;Store2
            说明: 该屏障确保Store1立刻刷新数据到内存(使其对其他处理器可见)的操作先于Store2及其后所有存储指令的操作
        > LoadStore Barriers
            指令示例: Load1;LoadStore;Store2
            说明: 确保Load1的数据装载先于Store2及其后所有的存储指令刷新数据到内存的操作
        > StoreLoad Barriers
        	指令示例: Store1;StoreLoad;Load2	
            说明: 该屏障确保Store1立刻刷新数据到内存的操作先于Load2及其后所有装载装载指令的操作。它会使该屏障之前的所有内存访问指令(存储指令和访问指令)完成之后,才执行该屏障之后的内存访问指令
            注意：
                该屏障同时具备其他三个屏障的效果，因此也称之为全能屏障（mfence），是目前大多数处理器所支持的；但是相对其他屏障，该屏障的开销相对昂贵。然而，除了mfence，不同的CPU架构对内存屏障的实现方式与实现程度非常不一样
    2) x86内存屏障(指令)
        > Store Barrier
            sfence指令，相当于StoreStore Barriers
            强制所有在sfence指令之前的store指令，都在该sfence指令执行之前被执行，发送缓存失效信号，并把store buffer中的数据刷出到CPU的L1 Cache中；所有在sfence指令之后的store指令，都在该sfence指令执行之后被执行。即，禁止对sfence指令前后store指令的重排序跨越sfence指令，使所有Store Barrier之前发生的内存更新都是可见的
        > Load Barrier
            lfence指令，相当于LoadLoad Barriers  
            强制所有在lfence指令之后的load指令，都在该lfence指令执行之后被执行，并且一直等到load buffer被该CPU读完才能执行之后的load指令（发现缓存失效后发起的刷入）。即，禁止对lfence指令前后load指令的重排序跨越lfence指令，配合Store Barrier，使所有Store Barrier之前发生的内存更新，对Load Barrier之后的load操作都是可见的
        > Full Barrier
            mfence指令，相当于StoreLoad Barriers
            mfence指令综合了sfence指令与lfence指令的作用，强制所有在mfence指令之前的store/load指令，都在该mfence指令执行之前被执行；所有在mfence指令之后的store/load指令，都在该mfence指令执行之后被执行。即，禁止对mfence指令前后store/load指令的重排序跨越mfence指令，使所有Full Barrier之前发生的操作，对所有Full Barrier之后的操作都是可见的


(6) 什么地方需要内存障碍？
    在正常操作下，一个单线程代码片段中内存操作重排序一般不会产生问题，仍然可以正常工作，即使是在一个SMP内核系统中也是如此。但是，下面四种场景下，重新排序可能会引发问题：
    > 多理器间的交互
    > 原子操作
    > 设备访问
    > 中断
```
4、内存屏障API
```
见[linux kernel 同步]
```

### memory order
```
内存的排序既可能发生在编译器编译期间，也可能发生在 CPU 指令执行期间
多线程环境下，指令的乱序执行会造成无法预测的行为

X86和ARM的memory ordering就截然不同









Memory Barrier来解决此问题

c++提供了6种标准的memory order实现

```

### std::atomic
```
```

### spin lock的优化
```
https://zhuanlan.zhihu.com/p/84617791?utm_source=qq&utm_medium=social&utm_oi=555400798499188736


```

